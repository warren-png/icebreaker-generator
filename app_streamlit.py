"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
APP STREAMLIT V28.6 - URLs FICHES DEPUIS LEONAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- URLs fiches de poste depuis Leonar (custom_text_1) ou manuelles
- Messages injectÃ©s dans custom_variable_1/2/3 (sÃ©quence auto)
- Backup dans notes (lisible)
- Pagination Leonar (rÃ©cupÃ¨re tous les prospects)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import streamlit as st
import requests
import os
import re
import json
import time
import anthropic
from datetime import datetime, timedelta
from dotenv import load_dotenv
from bs4 import BeautifulSoup

load_dotenv()

# ========================================
# CONFIGURATION
# ========================================

st.set_page_config(page_title="Icebreaker Generator V28.6", page_icon="ğŸ¯", layout="wide")

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")

try:
    LEONAR_EMAIL = st.secrets["LEONAR_EMAIL"]
    LEONAR_PASSWORD = st.secrets["LEONAR_PASSWORD"]
    LEONAR_CAMPAIGN_ID = st.secrets["LEONAR_CAMPAIGN_ID"]
except:
    LEONAR_EMAIL = os.getenv("LEONAR_EMAIL")
    LEONAR_PASSWORD = os.getenv("LEONAR_PASSWORD")
    LEONAR_CAMPAIGN_ID = os.getenv("LEONAR_CAMPAIGN_ID")

PROCESSED_FILE = "processed_prospects.txt"

# Session state
if 'leonar_prospects' not in st.session_state:
    st.session_state.leonar_prospects = []
if 'generation_stats' not in st.session_state:
    st.session_state.generation_stats = {'calls': 0, 'tokens': 0, 'cost': 0}


# ========================================
# LEONAR API
# ========================================

def get_leonar_token():
    """Obtient un token d'authentification Leonar"""
    try:
        r = requests.post(
            'https://dashboard.leonar.app/api/1.1/wf/auth',
            json={"email": LEONAR_EMAIL, "password": LEONAR_PASSWORD},
            timeout=10
        )
        return r.json()['response']['token'] if r.status_code == 200 else None
    except:
        return None


def get_new_prospects_leonar(token):
    """RÃ©cupÃ¨re les nouveaux prospects depuis Leonar (avec pagination)"""
    try:
        all_prospects = []
        cursor = 0
        page = 1
        
        # Paginer pour rÃ©cupÃ©rer TOUS les prospects
        while True:
            url = f'https://dashboard.leonar.app/api/1.1/obj/matching?constraints=[{{"key":"campaign","constraint_type":"equals","value":"{LEONAR_CAMPAIGN_ID}"}}]&cursor={cursor}&limit=100'
            
            r = requests.get(
                url,
                headers={'Authorization': f'Bearer {token}'},
                timeout=15
            )
            
            if r.status_code != 200:
                st.error(f"âŒ Leonar API erreur: status {r.status_code}")
                break
            
            data = r.json()
            results = data.get('response', {}).get('results', [])
            remaining = data.get('response', {}).get('remaining', 0)
            
            all_prospects.extend(results)
            st.info(f"ğŸ“Š Page {page}: {len(results)} prospects (total: {len(all_prospects)}, remaining: {remaining})")
            
            # S'il n'y a plus de rÃ©sultats, arrÃªter
            if not results or remaining == 0:
                break
            
            # Passer Ã  la page suivante
            cursor += len(results)
            page += 1
            
            # SÃ©curitÃ© : max 10 pages (1000 prospects)
            if page > 10:
                st.warning("âš ï¸ Limite de 1000 prospects atteinte")
                break
        
        st.info(f"ğŸ“Š Debug: {len(all_prospects)} prospects TOTAL trouvÃ©s dans Leonar")
        
        processed = load_processed()
        st.info(f"ğŸ“Š Debug: {len(processed)} prospects dÃ©jÃ  traitÃ©s dans le fichier")
        
        # Filtrer
        filtered = []
        for p in all_prospects:
            pid = p['_id']
            notes = p.get('notes', '')
            
            if pid in processed:
                continue  # DÃ©jÃ  traitÃ© (fichier local)
            
            if notes and len(notes) >= 100 and 'MESSAGE 1' in notes:
                continue  # DÃ©jÃ  traitÃ© (notes Leonar)
            
            filtered.append(p)
        
        st.success(f"âœ… {len(filtered)} prospects Ã  traiter aprÃ¨s filtrage")
        return filtered
        
    except Exception as e:
        st.error(f"Erreur Leonar: {e}")
        return []


def update_prospect_leonar(token, prospect_id, sequence_data):
    """Met Ã  jour le prospect dans Leonar avec la sÃ©quence gÃ©nÃ©rÃ©e"""
    try:
        # Backup dans les notes (lisible)
        formatted_notes = f"""â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OBJETS SUGGÃ‰RÃ‰S
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{sequence_data.get('subject_lines', '')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MESSAGE 1 (ICEBREAKER - J+0)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{sequence_data.get('message_1', '')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MESSAGE 2 (LA PROPOSITION - J+5)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{sequence_data.get('message_2', '')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MESSAGE 3 (BREAK-UP - J+12)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{sequence_data.get('message_3', '')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""

        # Envoi : notes (backup) + custom_variables (sÃ©quence auto)
        requests.patch(
            f'https://dashboard.leonar.app/api/1.1/obj/matching/{prospect_id}',
            headers={'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'},
            json={
                "notes": formatted_notes,
                "custom_variable_1": sequence_data.get('message_1', ''),
                "custom_variable_2": sequence_data.get('message_2', ''),
                "custom_variable_3": sequence_data.get('message_3', '')
            },
            timeout=10
        )
        return True
    except:
        return False


def load_processed():
    """Charge la liste des prospects dÃ©jÃ  traitÃ©s"""
    if os.path.exists(PROCESSED_FILE):
        with open(PROCESSED_FILE, 'r') as f:
            return set(f.read().splitlines())
    return set()


def save_processed(pid):
    """Sauvegarde un prospect comme traitÃ©"""
    with open(PROCESSED_FILE, 'a') as f:
        f.write(f"{pid}\n")


# ========================================
# APIFY - SCRAPING LINKEDIN
# ========================================

def init_apify_client():
    """Initialise le client Apify"""
    from apify_client import ApifyClient
    if not APIFY_API_TOKEN:
        raise ValueError("APIFY_API_TOKEN manquant")
    return ApifyClient(APIFY_API_TOKEN)


def scrape_linkedin_profile(apify_client, linkedin_url):
    """Scrape un profil LinkedIn"""
    try:
        run = apify_client.actor("dev_fusion/Linkedin-Profile-Scraper").call(
            run_input={"profileUrls": [linkedin_url]}
        )
        items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
        return items[0] if items else {}
    except:
        return {}


def scrape_linkedin_posts(apify_client, linkedin_url):
    """Scrape les posts LinkedIn"""
    try:
        run = apify_client.actor("supreme_coder/linkedin-post").call(
            run_input={
                "deepScrape": True,
                "limitPerSource": 10,
                "rawData": False,
                "urls": [linkedin_url]
            }
        )
        items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
        # Filtre strict 6 mois
        return filter_recent_posts(items, max_age_months=6)
    except:
        return []


def filter_recent_posts(posts, max_age_months=6):
    """
    Filtre les posts < 6 mois
    Si date non parsable â†’ on INCLUT le post (moins strict)
    """
    if not posts:
        return []
    
    cutoff = datetime.now() - timedelta(days=max_age_months * 30)
    recent = []
    
    for post in posts:
        if not isinstance(post, dict):
            continue
        
        # RÃ©cupÃ©rer la date - chercher TOUS les champs possibles
        date_str = (
            post.get('date') or 
            post.get('postedDate') or 
            post.get('postedAt') or
            post.get('timestamp') or
            post.get('publishedAt') or
            post.get('time') or
            post.get('posted') or
            post.get('datePosted') or
            ''
        )
        
        # Si pas de date trouvÃ©e, on INCLUT quand mÃªme le post
        # (approche permissive - mieux vaut un post potentiellement vieux qu'aucun post)
        if not date_str:
            recent.append(post)
            continue
        
        # Parser la date
        post_date = parse_date(date_str)
        
        # Si parsing Ã©choue, on inclut quand mÃªme
        if post_date is None:
            recent.append(post)
            continue
        
        # Si date rÃ©cente, on inclut
        if post_date >= cutoff:
            recent.append(post)
    
    return recent[:5]


def parse_date(date_str):
    """Parse une date avec plusieurs formats"""
    if not date_str:
        return None
    
    date_str = str(date_str).strip()
    
    # Formats standards
    formats = [
        '%Y-%m-%d',
        '%Y-%m-%dT%H:%M:%S',
        '%Y-%m-%dT%H:%M:%S.%fZ',
        '%Y-%m-%dT%H:%M:%SZ',
        '%d/%m/%Y',
        '%d-%m-%Y',
        '%Y/%m/%d',
        '%B %d, %Y',
        '%b %d, %Y',
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(date_str[:19], fmt)
        except:
            continue
    
    # Dates relatives ("2d ago", "3w ago", "il y a 2 jours")
    return parse_relative_date(date_str)


def parse_relative_date(date_str):
    """Parse les dates relatives - formats LinkedIn et autres"""
    if not date_str:
        return None
    
    date_str = date_str.lower().strip()
    now = datetime.now()
    
    # Patterns anglais et franÃ§ais
    patterns = [
        # Anglais complet
        (r'(\d+)\s*d(?:ay)?s?\s*ago', 'days'),
        (r'(\d+)\s*w(?:eek)?s?\s*ago', 'weeks'),
        (r'(\d+)\s*mo(?:nth)?s?\s*ago', 'months'),
        (r'(\d+)\s*h(?:our)?s?\s*ago', 'hours'),
        (r'(\d+)\s*yr?s?\s*ago', 'years'),
        # Anglais court (LinkedIn style: "1d", "2w", "3mo")
        (r'^(\d+)d$', 'days'),
        (r'^(\d+)w$', 'weeks'),
        (r'^(\d+)mo$', 'months'),
        (r'^(\d+)h$', 'hours'),
        (r'^(\d+)yr?$', 'years'),
        # Avec espace
        (r'(\d+)\s*d\b', 'days'),
        (r'(\d+)\s*w\b', 'weeks'),
        (r'(\d+)\s*mo\b', 'months'),
        (r'(\d+)\s*h\b', 'hours'),
        # FranÃ§ais
        (r'il y a (\d+)\s*jour', 'days'),
        (r'il y a (\d+)\s*semaine', 'weeks'),
        (r'il y a (\d+)\s*mois', 'months'),
        (r'il y a (\d+)\s*heure', 'hours'),
        (r'il y a (\d+)\s*an', 'years'),
        # "posted X days ago"
        (r'posted\s*(\d+)\s*d', 'days'),
        (r'posted\s*(\d+)\s*w', 'weeks'),
        (r'posted\s*(\d+)\s*mo', 'months'),
        # "X days" sans "ago"
        (r'^(\d+)\s*days?$', 'days'),
        (r'^(\d+)\s*weeks?$', 'weeks'),
        (r'^(\d+)\s*months?$', 'months'),
    ]
    
    for pattern, unit in patterns:
        match = re.search(pattern, date_str)
        if match:
            value = int(match.group(1))
            if unit == 'hours':
                return now - timedelta(hours=value)
            elif unit == 'days':
                return now - timedelta(days=value)
            elif unit == 'weeks':
                return now - timedelta(weeks=value)
            elif unit == 'months':
                return now - timedelta(days=value * 30)
            elif unit == 'years':
                return now - timedelta(days=value * 365)
    
    return None


# ========================================
# SCRAPING WEB (SERPER)
# ========================================

def search_web_prospect(full_name, company_name):
    """
    Recherche web sur le prospect via Serper
    Retourne les rÃ©sultats rÃ©cents (<6 mois)
    """
    if not SERPER_API_KEY:
        return []
    
    try:
        # Recherche actualitÃ©s rÃ©centes
        query = f'"{full_name}" "{company_name}" OR "{full_name}" finance'
        
        response = requests.post(
            'https://google.serper.dev/search',
            headers={
                'X-API-KEY': SERPER_API_KEY,
                'Content-Type': 'application/json'
            },
            json={
                'q': query,
                'num': 10,
                'tbs': 'qdr:m6'  # Derniers 6 mois
            },
            timeout=10
        )
        
        if response.status_code != 200:
            return []
        
        data = response.json()
        results = []
        
        # Organic results
        for item in data.get('organic', [])[:5]:
            results.append({
                'title': item.get('title', ''),
                'snippet': item.get('snippet', ''),
                'link': item.get('link', ''),
                'type': 'web'
            })
        
        # News results (souvent plus rÃ©cents)
        for item in data.get('news', [])[:3]:
            results.append({
                'title': item.get('title', ''),
                'snippet': item.get('snippet', ''),
                'link': item.get('link', ''),
                'date': item.get('date', ''),
                'type': 'news'
            })
        
        return results
        
    except Exception as e:
        print(f"Erreur Serper: {e}")
        return []


# ========================================
# SCRAPING FICHE DE POSTE - MULTI-SITES
# ========================================

def scrape_job_posting(url):
    """
    Scrape une fiche de poste depuis diffÃ©rents job boards
    Supporte : HelloWork, LinkedIn, Apec, Indeed, gÃ©nÃ©rique
    """
    if not url or not url.strip():
        return None
    
    url = url.strip()
    
    try:
        if "hellowork.com" in url:
            return scrape_hellowork(url)
        elif "linkedin.com/jobs" in url:
            return scrape_linkedin_job(url)
        elif "apec.fr" in url:
            return scrape_apec(url)
        elif "indeed.com" in url or "indeed.fr" in url:
            return scrape_indeed(url)
        else:
            return scrape_generic(url)
    except Exception as e:
        print(f"Erreur scraping: {e}")
        return scrape_generic(url)


def scrape_hellowork(url):
    """Scrape HelloWork"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return scrape_generic(url)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Titre - plusieurs sÃ©lecteurs possibles
        title = ""
        for selector in ['h1.tw-text-3xl', 'h1[data-cy="job-title"]', 'h1']:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        # Description - chercher dans plusieurs conteneurs
        description = ""
        for selector in ['div[data-cy="job-description"]', 'div.job-description', 'div.description', 'article', 'main']:
            elem = soup.select_one(selector)
            if elem:
                description = elem.get_text(separator='\n', strip=True)
                if len(description) > 200:
                    break
        
        # Fallback : tous les paragraphes
        if len(description) < 200:
            paragraphs = soup.find_all(['p', 'li'])
            description = '\n'.join([p.get_text(strip=True) for p in paragraphs])
        
        return {
            'title': title[:200],
            'description': description[:4000],
            'source': 'HelloWork',
            'url': url
        }
    except:
        return scrape_generic(url)


def scrape_linkedin_job(url):
    """Scrape LinkedIn Jobs"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return scrape_generic(url)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Titre
        title = ""
        for selector in ['h1.top-card-layout__title', 'h1.topcard__title', 'h1']:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        # Description
        description = ""
        for selector in ['div.show-more-less-html__markup', 'div.description__text', 'div.job-description']:
            elem = soup.select_one(selector)
            if elem:
                description = elem.get_text(separator='\n', strip=True)
                break
        
        return {
            'title': title[:200],
            'description': description[:4000],
            'source': 'LinkedIn',
            'url': url
        }
    except:
        return scrape_generic(url)


def scrape_apec(url):
    """Scrape Apec"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml',
            'Accept-Language': 'fr-FR,fr;q=0.9',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return scrape_generic(url)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Titre
        title = ""
        for selector in ['h1[data-cy="offerTitle"]', 'h1.offer-title', 'h1']:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        # Description - Apec structure spÃ©cifique
        description = ""
        
        # Chercher les sections de contenu
        content_sections = soup.select('div.offer-description, div.job-description, section.description, div[class*="description"]')
        for section in content_sections:
            text = section.get_text(separator='\n', strip=True)
            if len(text) > len(description):
                description = text
        
        # Fallback
        if len(description) < 200:
            main_content = soup.select_one('main') or soup.select_one('article')
            if main_content:
                description = main_content.get_text(separator='\n', strip=True)
        
        return {
            'title': title[:200],
            'description': description[:4000],
            'source': 'Apec',
            'url': url
        }
    except:
        return scrape_generic(url)


def scrape_indeed(url):
    """Scrape Indeed"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return scrape_generic(url)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Titre
        title = ""
        for selector in ['h1.jobsearch-JobInfoHeader-title', 'h1[data-testid="jobTitle"]', 'h1']:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                break
        
        # Description
        description = ""
        for selector in ['div#jobDescriptionText', 'div.jobsearch-jobDescriptionText', 'div[data-testid="jobDescription"]']:
            elem = soup.select_one(selector)
            if elem:
                description = elem.get_text(separator='\n', strip=True)
                break
        
        return {
            'title': title[:200],
            'description': description[:4000],
            'source': 'Indeed',
            'url': url
        }
    except:
        return scrape_generic(url)


def scrape_generic(url):
    """Scraping gÃ©nÃ©rique pour sites non supportÃ©s"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Supprimer scripts et styles
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # Titre
        title = ""
        h1 = soup.find('h1')
        if h1:
            title = h1.get_text(strip=True)
        
        # Description - prendre le contenu principal
        description = ""
        
        # Chercher le contenu principal
        main = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile('content|description|job', re.I))
        if main:
            description = main.get_text(separator='\n', strip=True)
        else:
            # Fallback : tous les paragraphes
            paragraphs = soup.find_all(['p', 'li'])
            description = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
        
        return {
            'title': title[:200],
            'description': description[:4000],
            'source': 'Generic',
            'url': url
        }
    except Exception as e:
        print(f"Erreur scraping gÃ©nÃ©rique: {e}")
        return None


# ========================================
# GÃ‰NÃ‰RATION V28 - UN SEUL APPEL CLAUDE
# ========================================

def generate_sequence_v28(prospect_data, posts_data, web_data, job_posting_data):
    """
    GÃ©nÃ¨re M1 + M2 en UN SEUL appel Claude
    IntÃ¨gre posts LinkedIn + rÃ©sultats web
    """
    
    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    
    # Extraire donnÃ©es
    prenom = get_firstname(prospect_data)
    titre_poste = get_job_title(job_posting_data)
    
    # Formater pour le prompt
    posts_formatted = format_posts(posts_data)
    web_formatted = format_web_results(web_data)
    profile_formatted = format_profile(prospect_data)
    fiche_formatted = job_posting_data.get('description', '') if job_posting_data else ''
    
    prompt = f"""Tu es chasseur de tÃªtes Finance chez Entourage Recrutement.
Tu dois gÃ©nÃ©rer 2 messages de prospection pour ce prospect.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DONNÃ‰ES PROSPECT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{profile_formatted}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
POSTS LINKEDIN RÃ‰CENTS (<6 mois uniquement)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{posts_formatted}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ACTUALITÃ‰S WEB RÃ‰CENTES (<6 mois uniquement)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{web_formatted}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FICHE DE POSTE : {titre_poste}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{fiche_formatted[:2500]}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GÃ‰NÃˆRE LES 2 MESSAGES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**MESSAGE 1 (Icebreaker)** - Structure EXACTE :

Bonjour {prenom},

[HOOK - CHOISIS UNE OPTION - PRIORITÃ‰ AUX INFOS RÃ‰CENTES :]
Option A (si un post LinkedIn OU une actualitÃ© web est pertinente) : 
  RÃ©fÃ©rence personnalisÃ©e (sujet PRÃ‰CIS, Ã©vÃ©nement, publication, nomination...)
  Puis transition vers le poste.
Option B (si aucune info rÃ©cente pertinente) :
  "Je vous contacte concernant votre recherche de {titre_poste}."

[PAIN POINT #1]
Identifie LA difficultÃ© principale de ce recrutement avec le VOCABULAIRE EXACT de la fiche.
Mentionne les compÃ©tences RARES demandÃ©es (rÃ©assurance, consolidation IFRS, provisions techniques, etc.)
PAS de gÃ©nÃ©ralitÃ©s ("rigueur", "agilitÃ©", "dynamisme").

Quels sont les principaux Ã©carts que vous observez entre vos attentes et les profils rencontrÃ©s ?

Bien Ã  vous,

---

**MESSAGE 2 (Relance avec profils)** - Structure EXACTE :

Bonjour {prenom},

Je me permets de vous relancer concernant votre recherche de {titre_poste}.

[PAIN POINT #2 - DIFFÃ‰RENT DE M1]
Autre angle sur une AUTRE difficultÃ©, autres compÃ©tences de la fiche.

J'ai identifiÃ© 2 profils qui pourraient retenir votre attention :

- L'un [PROFIL 1 : spÃ©cialiste avec compÃ©tences EXACTES de la fiche, expÃ©rience cohÃ©rente]

- L'autre [PROFIL 2 : parcours DIFFÃ‰RENT mais pertinent, PAS "Big 4" par dÃ©faut]

Seriez-vous d'accord pour recevoir leurs synthÃ¨ses anonymisÃ©es ?

Bien Ã  vous,

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
INTERDICTIONS ABSOLUES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âŒ "Je travaille sur...", "Je travaille actuellement..."
âŒ "rigueur", "agilitÃ©", "dynamisme", "dynamique", "croissance"
âŒ Inventer des compÃ©tences/certifications NON dans la fiche
âŒ RÃ©pÃ©ter le MÃŠME pain point entre M1 et M2
âŒ Utiliser des informations datant de plus de 6 mois
âŒ Profils incohÃ©rents avec la fiche

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FORMAT DE RÃ‰PONSE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
---MESSAGE_1---
[contenu message 1]
---MESSAGE_2---
[contenu message 2]
"""

    try:
        # Retry avec backoff exponentiel pour rate limit
        max_retries = 3
        base_delay = 30  # secondes
        
        for attempt in range(max_retries):
            try:
                message = client.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=1500,
                    messages=[{"role": "user", "content": prompt}]
                )
                break  # SuccÃ¨s, sortir de la boucle
            except anthropic.RateLimitError as e:
                if attempt < max_retries - 1:
                    wait_time = base_delay * (2 ** attempt)  # 30s, 60s, 120s
                    st.warning(f"â³ Rate limit atteint. Attente {wait_time}s avant retry ({attempt + 1}/{max_retries})...")
                    time.sleep(wait_time)
                else:
                    raise e  # DerniÃ¨re tentative Ã©chouÃ©e, propager l'erreur
        
        # Stats
        st.session_state.generation_stats['calls'] += 1
        st.session_state.generation_stats['tokens'] += message.usage.input_tokens + message.usage.output_tokens
        st.session_state.generation_stats['cost'] += (message.usage.input_tokens * 0.003 + message.usage.output_tokens * 0.015) / 1000
        
        result = message.content[0].text.strip()
        m1, m2 = parse_messages(result)
        m3 = generate_message_3(prenom)
        subject_lines = generate_subject_lines(titre_poste)
        
        return {
            'subject_lines': subject_lines,
            'message_1': m1,
            'message_2': m2,
            'message_3': m3
        }
        
    except Exception as e:
        st.error(f"Erreur Claude: {e}")
        return None


def parse_messages(response):
    """Parse la rÃ©ponse Claude"""
    if '---MESSAGE_1---' in response and '---MESSAGE_2---' in response:
        parts = response.split('---MESSAGE_2---')
        m1 = parts[0].replace('---MESSAGE_1---', '').strip()
        m2 = parts[1].strip() if len(parts) > 1 else ""
    else:
        lines = response.split('\n\n')
        mid = len(lines) // 2
        m1 = '\n\n'.join(lines[:mid])
        m2 = '\n\n'.join(lines[mid:])
    return m1, m2


def generate_message_3(prenom):
    """Message 3 - Template fixe"""
    return f"""Bonjour {prenom},

Je comprends que vous n'ayez pas eu le temps de revenir vers moi â€” je sais Ã  quel point vos fonctions sont sollicitÃ©es.

Avant de clore le dossier de mon cÃ´tÃ©, une derniÃ¨re question : Est-ce que le timing n'est simplement pas bon pour l'instant, ou bien travaillez-vous dÃ©jÃ  avec d'autres cabinets/recruteurs sur ce poste ?

Si c'est une question de timing, je serai ravi de reprendre contact dans quelques semaines.

Si vous prÃ©fÃ©rez gÃ©rer ce recrutement autrement, aucun souci â€” je vous souhaite de trouver la perle rare rapidement.

Merci en tous cas pour votre attention,

Bonne continuation,"""


def generate_subject_lines(titre_poste):
    """GÃ©nÃ¨re les objets d'email"""
    return f"""1. {titre_poste} - profils qualifiÃ©s ?
2. Re: {titre_poste}
3. Question rapide sur votre recrutement"""


# ========================================
# UTILITAIRES
# ========================================

def get_firstname(prospect_data):
    """Extrait le prÃ©nom"""
    for key in ['first_name', 'firstname', 'prÃ©nom']:
        if prospect_data.get(key):
            return str(prospect_data[key]).strip().capitalize()
    
    full_name = prospect_data.get('full_name') or prospect_data.get('user_full name', '')
    if full_name and ' ' in str(full_name):
        return str(full_name).split()[0].capitalize()
    return "[PrÃ©nom]"


def get_job_title(job_posting_data):
    """Extrait le titre du poste"""
    if not job_posting_data:
        return "[Poste]"
    title = job_posting_data.get('title', '')
    title = re.sub(r'\s*\(?[HhFf]\s*[/\-]\s*[HhFfMm]\)?', '', title)
    return title.strip() or "[Poste]"


def format_posts(posts):
    """Formate les posts LinkedIn pour le prompt"""
    if not posts:
        return "Aucun post LinkedIn rÃ©cent trouvÃ©."
    
    formatted = []
    for i, post in enumerate(posts[:5], 1):
        # Chercher le texte dans plusieurs champs possibles
        text = (
            post.get('text') or 
            post.get('postText') or 
            post.get('content') or 
            post.get('commentary') or
            post.get('description') or
            post.get('body') or
            ''
        )[:500]
        
        # Chercher la date
        date = (
            post.get('date') or 
            post.get('postedDate') or 
            post.get('postedAt') or
            post.get('time') or
            post.get('timestamp') or
            'Date inconnue'
        )
        
        # RÃ©cupÃ©rer les interactions si disponibles
        reactions = post.get('numLikes') or post.get('reactions') or post.get('likes') or ''
        comments = post.get('numComments') or post.get('comments') or ''
        
        stats = ""
        if reactions or comments:
            stats = f" | ğŸ‘{reactions} ğŸ’¬{comments}"
        
        if text:
            formatted.append(f"POST {i} ({date}{stats}):\n{text}")
    
    if not formatted:
        return "Aucun post LinkedIn avec contenu trouvÃ©."
    
    return "\n\n".join(formatted)


def format_web_results(web_data):
    """Formate les rÃ©sultats web pour le prompt"""
    if not web_data:
        return "Aucune actualitÃ© web rÃ©cente trouvÃ©e."
    
    formatted = []
    for i, item in enumerate(web_data[:5], 1):
        title = item.get('title', '')
        snippet = item.get('snippet', '')
        date = item.get('date', '')
        item_type = item.get('type', 'web')
        formatted.append(f"[{item_type.upper()}] {title}\n{snippet}\n({date})")
    return "\n\n".join(formatted)


def format_profile(prospect_data):
    """Formate le profil pour le prompt"""
    return f"""Nom: {prospect_data.get('full_name') or prospect_data.get('user_full name', 'N/A')}
Titre: {prospect_data.get('headline') or prospect_data.get('linkedin_headline', 'N/A')}
Entreprise: {prospect_data.get('company') or prospect_data.get('linkedin_company', 'N/A')}"""


def extract_prospect_data(leonar_prospect):
    """Extrait les donnÃ©es du prospect Leonar"""
    full_name = leonar_prospect.get('user_full name', '')
    first_name = ''
    
    if full_name and ' ' in str(full_name):
        first_name = str(full_name).split()[0]
    
    return {
        '_id': leonar_prospect.get('_id', ''),
        'full_name': full_name,
        'user_full name': full_name,
        'first_name': first_name,
        'company': leonar_prospect.get('linkedin_company', ''),
        'linkedin_company': leonar_prospect.get('linkedin_company', ''),
        'linkedin_url': leonar_prospect.get('linkedin_url', ''),
        'headline': leonar_prospect.get('linkedin_headline', ''),
        'linkedin_headline': leonar_prospect.get('linkedin_headline', '')
    }


# ========================================
# INTERFACE
# ========================================

st.title("ğŸ¯ Icebreaker Generator V28.6")
st.caption("Leonar + Scraping LinkedIn/Web + GÃ©nÃ©ration IA")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    if ANTHROPIC_API_KEY:
        st.success("âœ… Anthropic API")
    else:
        st.error("âŒ ANTHROPIC_API_KEY")
    
    if APIFY_API_TOKEN:
        st.success("âœ… Apify API")
    else:
        st.error("âŒ APIFY_API_TOKEN")
    
    if SERPER_API_KEY:
        st.success("âœ… Serper API (web)")
    else:
        st.warning("âš ï¸ SERPER_API_KEY (optionnel)")
    
    if all([LEONAR_EMAIL, LEONAR_PASSWORD, LEONAR_CAMPAIGN_ID]):
        if get_leonar_token():
            st.success("âœ… Leonar connectÃ©")
        else:
            st.error("âŒ Erreur Leonar")
    else:
        st.warning("âš ï¸ Config Leonar incomplÃ¨te")
    
    st.divider()
    st.header("ğŸ“Š Stats session")
    st.metric("Appels API", st.session_state.generation_stats['calls'])
    st.metric("Tokens", f"{st.session_state.generation_stats['tokens']:,}")
    st.metric("CoÃ»t", f"${st.session_state.generation_stats['cost']:.4f}")


# Onglets
tab1, tab2 = st.tabs(["ğŸš€ GÃ©nÃ©ration Leonar", "ğŸ§ª Test Manuel"])


# ========================================
# TAB 1 : GÃ‰NÃ‰RATION LEONAR
# ========================================
with tab1:
    st.header("GÃ©nÃ©ration automatique depuis Leonar")
    
    if not all([LEONAR_EMAIL, LEONAR_PASSWORD, LEONAR_CAMPAIGN_ID]):
        st.error("Configuration Leonar manquante dans .env ou secrets")
        st.stop()
    
    token = get_leonar_token()
    if not token:
        st.error("Impossible de se connecter Ã  Leonar")
        st.stop()
    
    # Zone URLs fiches de poste - AGRANDIE
    st.subheader("ğŸ“„ URLs des fiches de poste (optionnel)")
    st.caption("ğŸ’¡ PrioritÃ© : URL dans Leonar (`custom_text_1`) > URL ci-dessous. Si tu remplis `custom_text_1` dans Leonar, tu peux laisser vide ici.")
    
    job_urls_input = st.text_area(
        "URLs (une par ligne)",
        height=200,
        placeholder="https://www.hellowork.com/...\nhttps://www.apec.fr/...\nhttps://www.linkedin.com/jobs/..."
    )
    
    # Parser les URLs
    job_urls_list = []
    has_apec_urls = False
    if job_urls_input:
        job_urls_list = [u.strip() for u in job_urls_input.strip().split('\n') if u.strip()]
        st.info(f"âœ… {len(job_urls_list)} URL(s) dÃ©tectÃ©e(s)")
        
        # DÃ©tecter URLs Apec
        apec_urls = [u for u in job_urls_list if 'apec.fr' in u.lower()]
        if apec_urls:
            has_apec_urls = True
            st.warning(f"âš ï¸ {len(apec_urls)} URL(s) Apec dÃ©tectÃ©e(s). Le scraping Apec ne fonctionne pas (JavaScript). Collez le texte ci-dessous.")
    
    # Champ fallback pour Apec
    apec_manual_description = ""
    if has_apec_urls:
        apec_manual_description = st.text_area(
            "ğŸ“‹ Description Apec (coller le texte de la fiche)",
            height=300,
            placeholder="Copiez-collez ici le contenu de la fiche de poste Apec...\n\nAstuce : Sur la page Apec, sÃ©lectionnez tout le texte de la description et collez-le ici."
        )
    
    # RafraÃ®chir prospects
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        if st.button("ğŸ”„ RafraÃ®chir", type="secondary"):
            with st.spinner("Chargement depuis Leonar..."):
                st.session_state.leonar_prospects = get_new_prospects_leonar(token)
    
    with col2:
        if st.button("ğŸ—‘ï¸ Reset traitÃ©s", type="secondary"):
            if os.path.exists(PROCESSED_FILE):
                os.remove(PROCESSED_FILE)
                st.success("âœ… Liste des prospects traitÃ©s effacÃ©e")
            with st.spinner("Rechargement..."):
                st.session_state.leonar_prospects = get_new_prospects_leonar(token)
    
    with col3:
        if st.session_state.leonar_prospects:
            st.success(f"âœ… {len(st.session_state.leonar_prospects)} prospects Ã  traiter")
        else:
            st.warning("âš ï¸ 0 prospect - Cliquez sur RafraÃ®chir")
    
    # Liste des prospects
    if st.session_state.leonar_prospects:
        with st.expander(f"ğŸ‘¥ Voir les {len(st.session_state.leonar_prospects)} prospects", expanded=True):
            for i, p in enumerate(st.session_state.leonar_prospects):
                name = p.get('user_full name', 'Inconnu')
                company = p.get('linkedin_company', 'N/A')
                has_linkedin = "âœ…" if p.get('linkedin_url') else "âŒ"
                
                # URL fiche : prioritÃ© Leonar (custom_text_1) > manuelle
                leonar_url = p.get('custom_text_1', '').strip()
                manual_url = job_urls_list[i] if (job_urls_list and i < len(job_urls_list)) else None
                
                if leonar_url:
                    has_url = "ğŸ“„ URL Leonar âœ…"
                elif manual_url:
                    has_url = f"ğŸ“„ URL manuelle"
                else:
                    has_url = "âš ï¸ Pas d'URL"
                
                st.write(f"{i+1}. **{name}** | {company} | LinkedIn: {has_linkedin} | {has_url}")
        
        # Bouton gÃ©nÃ©ration
        if st.button("ğŸš€ LANCER LA GÃ‰NÃ‰RATION", type="primary", use_container_width=True):
            
            # VÃ©rifier qu'on a des URLs (Leonar ou manuelles)
            has_any_url = False
            for i, p in enumerate(st.session_state.leonar_prospects):
                leonar_url = p.get('custom_text_1', '').strip()
                manual_url = job_urls_list[i] if (job_urls_list and i < len(job_urls_list)) else None
                if leonar_url or manual_url:
                    has_any_url = True
                    break
            
            if not has_any_url:
                st.error("âš ï¸ Aucune URL de fiche de poste. Ajoutez-les dans Leonar (custom_text_1) ou collez-les ci-dessus.")
                st.stop()
            
            # Init Apify
            try:
                apify_client = init_apify_client()
            except Exception as e:
                st.error(f"Erreur Apify: {e}")
                st.stop()
            
            # Progress
            progress = st.progress(0)
            status = st.empty()
            
            # Traiter chaque prospect
            for i, prospect in enumerate(st.session_state.leonar_prospects):
                progress.progress((i + 1) / len(st.session_state.leonar_prospects))
                
                name = prospect.get('user_full name', 'Inconnu')
                company = prospect.get('linkedin_company', '')
                status.write(f"âš™ï¸ Traitement de **{name}**...")
                
                try:
                    # Extraire donnÃ©es prospect
                    p_data = extract_prospect_data(prospect)
                    
                    # URL fiche de poste : prioritÃ© Leonar (custom_text_1) > manuelle
                    leonar_url = prospect.get('custom_text_1', '').strip()
                    manual_url = job_urls_list[i] if (job_urls_list and i < len(job_urls_list)) else None
                    
                    if leonar_url:
                        job_url = leonar_url
                        st.caption(f"   ğŸ“„ URL depuis Leonar")
                    elif manual_url:
                        job_url = manual_url
                        st.caption(f"   ğŸ“„ URL manuelle")
                    else:
                        st.warning(f"   âš ï¸ Pas d'URL pour ce prospect - ignorÃ©")
                        continue
                    
                    # 1. Scraper la fiche de poste (ou utiliser description manuelle pour Apec)
                    job_data = None
                    is_apec_url = 'apec.fr' in job_url.lower()
                    
                    if is_apec_url and apec_manual_description:
                        # Utiliser la description manuelle pour Apec
                        job_data = {
                            'title': 'Poste Apec',
                            'description': apec_manual_description,
                            'source': 'Apec (manuel)',
                            'url': job_url
                        }
                        st.caption(f"   âœ… Description Apec (manuelle)")
                    elif is_apec_url and not apec_manual_description:
                        st.warning(f"   âš ï¸ URL Apec dÃ©tectÃ©e mais pas de description manuelle")
                    else:
                        with st.spinner(f"ğŸ“„ Scraping fiche de poste..."):
                            job_data = scrape_job_posting(job_url)
                            if job_data:
                                st.caption(f"   âœ… Fiche: {job_data.get('title', 'N/A')[:40]}...")
                    
                    # 2. Scraper LinkedIn posts
                    posts = []
                    if p_data.get('linkedin_url'):
                        with st.spinner(f"ğŸ” Scraping LinkedIn..."):
                            posts = scrape_linkedin_posts(apify_client, p_data['linkedin_url'])
                            st.caption(f"   âœ… {len(posts)} posts LinkedIn (<6 mois)")
                    
                    # 3. Recherche web
                    web_results = []
                    if SERPER_API_KEY and name != 'Inconnu':
                        with st.spinner(f"ğŸŒ Recherche web..."):
                            web_results = search_web_prospect(name, company)
                            st.caption(f"   âœ… {len(web_results)} rÃ©sultats web")
                    
                    # 4. GÃ©nÃ©rer sÃ©quence
                    with st.spinner(f"âœ¨ GÃ©nÃ©ration messages..."):
                        sequence = generate_sequence_v28(p_data, posts, web_results, job_data)
                    
                    if sequence:
                        # Update Leonar
                        if update_prospect_leonar(token, prospect['_id'], sequence):
                            save_processed(prospect['_id'])
                            st.toast(f"âœ… {name}")
                        else:
                            st.warning(f"âš ï¸ Erreur export Leonar pour {name}")
                    else:
                        st.error(f"âŒ Erreur gÃ©nÃ©ration pour {name}")
                
                except Exception as e:
                    st.error(f"âŒ Erreur pour {name}: {e}")
                
                # Pause anti-rate-limit entre chaque prospect (sauf le dernier)
                if i < len(st.session_state.leonar_prospects) - 1:
                    status.write(f"â³ Pause anti-rate-limit (3s)...")
                    time.sleep(3)
            
            progress.progress(1.0)
            status.empty()
            st.success("âœ… GÃ©nÃ©ration terminÃ©e !")
            st.balloons()
            
            # Reset liste
            st.session_state.leonar_prospects = []


# ========================================
# TAB 2 : TEST MANUEL
# ========================================
with tab2:
    st.header("Test manuel")
    
    col1, col2 = st.columns(2)
    
    with col1:
        t_prenom = st.text_input("PrÃ©nom", "Alexandre")
        t_nom = st.text_input("Nom", "Dupont")
        t_company = st.text_input("Entreprise", "CAMCA")
        t_linkedin = st.text_input("URL LinkedIn (optionnel)")
    
    with col2:
        t_job_url = st.text_input("URL fiche de poste")
        t_headline = st.text_input("Titre LinkedIn", "Responsable ComptabilitÃ©")
    
    if st.button("ğŸš€ GÃ©nÃ©rer", type="primary"):
        
        # PrÃ©parer donnÃ©es
        prospect = {
            'first_name': t_prenom,
            'full_name': f"{t_prenom} {t_nom}",
            'company': t_company,
            'headline': t_headline,
            'linkedin_url': t_linkedin
        }
        
        # Scraper fiche
        job_data = None
        if t_job_url:
            with st.spinner("ğŸ“„ Scraping fiche de poste..."):
                job_data = scrape_job_posting(t_job_url)
                if job_data:
                    st.success(f"âœ… Fiche: {job_data.get('title', '')[:50]}")
        
        # Scraper LinkedIn
        posts = []
        if t_linkedin:
            try:
                apify_client = init_apify_client()
                with st.spinner("ğŸ” Scraping LinkedIn..."):
                    posts = scrape_linkedin_posts(apify_client, t_linkedin)
                    st.success(f"âœ… {len(posts)} posts LinkedIn (<6 mois)")
            except Exception as e:
                st.warning(f"Scraping LinkedIn Ã©chouÃ©: {e}")
        
        # Recherche web
        web_results = []
        if SERPER_API_KEY:
            with st.spinner("ğŸŒ Recherche web..."):
                web_results = search_web_prospect(f"{t_prenom} {t_nom}", t_company)
                st.success(f"âœ… {len(web_results)} rÃ©sultats web")
        
        # GÃ©nÃ©rer
        with st.spinner("âœ¨ GÃ©nÃ©ration..."):
            sequence = generate_sequence_v28(prospect, posts, web_results, job_data)
        
        if sequence:
            st.divider()
            
            st.subheader("ğŸ“§ Objets")
            st.code(sequence['subject_lines'])
            
            st.subheader("âœ‰ï¸ Message 1")
            st.info(sequence['message_1'])
            
            st.subheader("âœ‰ï¸ Message 2")
            st.info(sequence['message_2'])
            
            st.subheader("âœ‰ï¸ Message 3")
            st.info(sequence['message_3'])
